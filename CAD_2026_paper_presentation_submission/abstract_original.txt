GPT-Based Assisted Editing of Pre-Publication Academic Writing: An Additive Multi-Dimensional Analysis

American Association of Corpus Linguistics Conference 2026

Rogério Yamada

Abstract

Non-English-speaking researchers are expected to publish in English, yet many struggle to do so given the challenges involved in writing quality academic English (Ädel & Erman, 2012; Bardi, 2015; Baumvol, Sarmento, & Da Luz Fontes, 2021; Belcher, 2007; Biber & Barbieri, 2007; Cargill & Burgess, 2008; Flowerdew, 2012; Pang, 2010; Ventura, 2024; Wray, 2019). Given that Large Language Models (LLMs) have been extensively trained on academic English, they may be leveraged as a form of English for Academic Purposes (EAP) literacy broker to edit pieces of writing so that they achieve the level of English required for publication (Flowerdew, 2012; Lillis & Curry, 2006). However, evidence from prior research suggests Artificial Intelligence (AI) may not replicate human academic writing lexicogrammar (Berber Sardinha, 2024). To explore whether AI can edit academic writing in a way that brings it closer to a publication-level standard, we collected a corpus of pre-publication articles and prompted GPT to improve the writing with a view to publication. The resulting corpus was then analysed using Multi-Dimensional Analysis (Biber, 1988, 1995); more specifically, we conducted an additive Multi-Dimensional Analysis (Berber Sardinha, Pinto, Mayer, Zuppardi, & Kauffmann, 2019) based on the dimensions of variation from Biber (1988). We compared the pre-publication articles (original and AI-processed) to articles published in quality journals. Both corpora (pre-publication and published) were sampled from works written prior to the advent of ChatGPT, matching the same time period and the same academic disciplines. The pre-publication corpus included 153 articles, totalling 746,066 tokens, while the published corpus contained 195 articles, totalling 1,789,557 tokens. The results indicated that AI-assisted academic writing diverged from human standards of academic English, as it amplified informational production characteristics (Dimension 1), explicit reference (Dimension 3), and abstraction (Dimension 5), while adjusting narrativity (Dimension 2). The effect of AI interventions varied across disciplines. Excessive informational production characteristics appeared less critical for the Health and Biological Sciences, whereas excessive abstraction was a concern for all disciplines except Applied Social Sciences. In general, AI tended to model all disciplines after the Health and Biological Sciences (i.e. “hard” sciences), in terms of informational production, and against the Applied Social Sciences in terms of abstract style. The effect on narrativity (Dimension 2) and argumentation (Dimension 4) varied according to discipline. In conclusion, this study found evidence of AI failing to reproduce human-authored academic English faithfully.
