AI Discourses on Climate Change: A Lexical Multidimensional Analysis of LLMs

Large Language Models (LLMs) have been producing and reproducing text that circulates widely across the social sphere. As they spread, they undergo a process of progressive naturalisation, gradually becoming incorporated into the human discursive repertoire. Understanding discourse as ideologically and culturally driven representations of real-world phenomena that are socially shared, situated, produced within social practice and capable of generating meaning (Biber and Egbert; Berber Sardinha and Fitzsimmons-Doolan), text generated by LLMs may not only reflect or reiterate dominant voices (Hughes; Gillings, Kohn, and Mautner), but also may express, for instance, social identity biases (Hu et al.). Their outputs may compete with, reinforce, or even replace discourses produced by human social groups, thus interfering with the collective construction of meaning on matters of public relevance, such as issues related to climate change. Further exploring the effect of LLMs on climate change discourses, this study looks at (i) which discourses are conveyed in human-authored registers about climate change; (ii) which discourses are produced by LLMs when prompted to generate the same registers; and (iii) what consonances (alignments or reinforcement of meanings) and tensions (divergences, shifts, or discursive resistances) surfaces in the comparison between human-authored and synthetic discourses. The study is fundamentally based on the grounds of Lexical Multi-Dimensional Analysis (LMDA) (Berber Sardinha and Fitzsimmons-Doolan), an extension of the Multi-Dimensional Analysis (Biber) that enables the analysis of latent discursive constructs through identifying lexical dimensions or sets of correlated lexical features. A reference corpus of human-authored texts (before the advent of ChatGPT) encompassing a wide range of registers from the press, governmental, non-governmental, and civil society was designed for the experiment. Several LLMs (ChatGPT, Gemini, and Grok) were prompted to generate corresponding text for each one present in the reference corpus, resulting in comparable target corpora of synthetic texts per LLM. Each target corpus’s discursive dimensional profile was identified via LMDA and compared with that of the reference corpus. Overall, the results revealed significant discursive differences between the synthetic registers and the human-authored ones, with variation per LLM, reflecting specific ideological positions towards climate change embedded in their training data. The dimensional profiles obtained from the analysis will be detailed in the presentation.

Keywords: Large Language Models (LLMs), climate change discourse, Lexical Multi-Dimensional Analysis (LMDA), human versus synthetic texts

References

Berber Sardinha, Tony, and Shannon Fitzsimmons-Doolan. Lexical Multi-Dimensional Analysis: Identifying Discourses and Ideologies. Elements in Corpus Linguistics. Cambridge: Cambridge University Press, July 2, 2025. isbn: 978-1-009-59843-9 978-1-009-33569-0 978-1-009-33568-3. https://www.cambridge.org/core/elements/lexical-multidimensional-analysis/B2321B62435360F4F7C4AF3
Biber, Douglas. Variation across Speech and Writing. 1st ed. Cambridge: Cambridge University Press, 1988. isbn: 978-0-521-32071-9. https://doi.org/10.1017/CBO9780511621024.
Biber, Douglas, and Jesse Egbert. “What Is a Register?: Accounting for Linguistic and Situational Variation within – and Outside of – Textual Varieties”. Register Studies 5, no. 1 (June 8, 2023): 1–22. issn: 2542-9477, 2542-9485, visited on 09/16/2023. https://doi.org/10.1075/rs.00004.bib. http://www.jbe-platform.com/content/journals/10.1075/rs.00004.bib.
Gillings, Mathew, Tobias Kohn, and Gerlinde Mautner. “The Rise of Large Language Models: Challenges for Critical Discourse Studies”. Critical Discourse Studies 22, no. 6 (Nov. 2, 2025): 625–641. issn: 1740-5904, 1740-5912, visited on 11/11/2025. https://doi.org/10.1080/17405904.2024.2373733. https://www.tandfonline.com/doi/full/10.1080/17405904.2024.2373733.
Hu, Tiancheng, et al. “Generative Language Models Exhibit Social Identity Biases”. Nature Computational Science 5, no. 1 (Dec. 12, 2024): 65–75. issn: 2662-8457, visited on 11/11/2025. https://doi.org/10.1038/s43588-024-00741-1. https://www.nature.com/articles/s43588-024-00741-1.
Hughes, Hannah. The IPCC and the Politics of Writing Climate Change. 1st ed. Cambridge University Press, June 30, 2024. isbn: 978-1-009-34155-4 978-1-009-34153-0, visited on 11/11/2025. https://doi.org/10.1017/9781009341554. https://www.cambridge.org/core/product/identifier/9781009341554/type/book.
