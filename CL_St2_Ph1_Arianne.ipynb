{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810d9f10-ec19-4b09-8f90-e983e460b319",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://laelgelcpublic.s3.sa-east-1.amazonaws.com/lael_50_years_narrow_white.png.no_years.400px_96dpi.png\" width=\"300\" alt=\"LAEL 50 years logo\">\n",
    "<h3>APPLIED LINGUISTICS GRADUATE PROGRAMME (LAEL)</h3>\n",
    "</center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c2c96-2fc3-4a1a-995b-c388036a2a15",
   "metadata": {},
   "source": "# Corpus Linguistics - Study 2 - Phase 1 - Arianne"
  },
  {
   "cell_type": "markdown",
   "id": "a685d8b0-7715-45a6-9489-2d3db9b346c8",
   "metadata": {},
   "source": [
    "## Required Python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e16e73-b1b9-4838-8cce-a29dc300868e",
   "metadata": {},
   "source": [
    "- <>\n",
    "- <>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa922755-c4d6-4008-9aad-d35e33b18ed7",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:43:41.475909Z",
     "start_time": "2025-08-15T17:43:40.381676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.edge.options import Options"
   ],
   "id": "fe5c4b76ea4f95d0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define input variables",
   "id": "b1911dcf5a8425df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:43:43.143113Z",
     "start_time": "2025-08-15T17:43:43.139966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_directory = 'cl_st2_ph1_arianne'\n",
    "output_directory = 'cl_st2_ph1_arianne'"
   ],
   "id": "ffccb3260540bf52",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create output directory",
   "id": "cb6c223aee4c74b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:43:45.565775Z",
     "start_time": "2025-08-15T17:43:45.560026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if the output directory already exists. If it does, do nothing. If it doesn't exist, create it.\n",
    "if os.path.exists(output_directory):\n",
    "    print('Output directory already exists.')\n",
    "else:\n",
    "    try:\n",
    "        os.makedirs(output_directory)\n",
    "        print('Output directory successfully created.')\n",
    "    except OSError as e:\n",
    "        print('Failed to create the directory:', e)\n",
    "        sys.exit(1)"
   ],
   "id": "3cc952da8ef9ec1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory already exists.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up logging",
   "id": "d70d1d62f24b7b24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:43:47.783195Z",
     "start_time": "2025-08-15T17:43:47.775986Z"
    }
   },
   "cell_type": "code",
   "source": "log_filename = f\"{output_directory}/{output_directory}.log\"",
   "id": "b3ddac3c2e754a25",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:43:49.114694Z",
     "start_time": "2025-08-15T17:43:49.109156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename=log_filename\n",
    ")"
   ],
   "id": "35dcc9ad97a25ab",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Functions",
   "id": "942249a4815dda54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create output subdirectories",
   "id": "e3f0ccb2a4a2d50c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:43:53.064426Z",
     "start_time": "2025-08-15T17:43:53.060867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_directory(path):\n",
    "    \"\"\"Creates a subdirectory if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "            print(f\"Successfully created the directory: {path}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Failed to create the {path} directory: {e}\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(f\"Directory already exists: {path}\")"
   ],
   "id": "cdbf413aac1b7b7e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Scrape web pages",
   "id": "bae051d781151e8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:43:55.906770Z",
     "start_time": "2025-08-15T17:43:55.898250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def scrape_html(url):\n",
    "    \"\"\"Loads a web page and returns its source HTML.\"\"\"\n",
    "    # Setting up the WebDriver\n",
    "    #service = Service(r'C:\\Users\\eyamr\\OneDrive\\00-Technology\\msedgedriver\\edgedriver_win64\\msedgedriver.exe')\n",
    "    service = Service('/Users/eyamrog/msedgedriver/edgedriver_mac64/msedgedriver')\n",
    "    #service = Service('/home/eyamrog/msedgedriver/edgedriver_linux64/msedgedriver')\n",
    "\n",
    "    # Configure Edge to run headless\n",
    "    options = Options()\n",
    "    # For modern Edge/Chromium; if incompatible with your version, try \"--headless\"\n",
    "    options.add_argument('--headless=new')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "\n",
    "    driver = webdriver.Edge(service=service, options=options)\n",
    "    html = None\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Explicit wait for stable page load\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        max_wait_time = 30\n",
    "        start_time = time.time()\n",
    "        previous_html = ''\n",
    "\n",
    "        while True:\n",
    "            current_html = driver.page_source\n",
    "            if current_html == previous_html or time.time() - start_time > max_wait_time:\n",
    "                break\n",
    "            previous_html = current_html\n",
    "            time.sleep(2)\n",
    "\n",
    "        html = driver.page_source  # Capture page source\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping {url}: {e}\")\n",
    "    finally:\n",
    "        # Always close WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "    return html"
   ],
   "id": "5d0f699053e28d5b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:43:59.802615Z",
     "start_time": "2025-08-15T17:43:59.794557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def scrape_html_docs2(df, path):\n",
    "    \"\"\"Iterates over a DataFrame and saves HTML pages within multiple WebDriver sessions.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError as e:\n",
    "            logging.error(f\"Failed to create the {path} directory: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Scraping HTML documents\"):\n",
    "        url = row['URL']\n",
    "        doc_id = row['ID']\n",
    "        filename = os.path.join(path, f\"{doc_id}.html\")\n",
    "\n",
    "        page_source = scrape_html(url)  # Call scrape_html function\n",
    "\n",
    "        if page_source:\n",
    "            with open(filename, 'w', encoding='utf-8') as file:\n",
    "                file.write(page_source)\n",
    "            logging.info(f\"Saved: {filename}\")"
   ],
   "id": "9f931a641f74c5b5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scraping [Greenpeace Stories](https://www.greenpeace.org/international/story/)",
   "id": "7ba455474af4bc4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define local variables",
   "id": "1a800858be531266"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:44:02.940014Z",
     "start_time": "2025-08-15T17:44:02.933701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "id = 'grp'\n",
    "path = os.path.join(output_directory, id)"
   ],
   "id": "8b141ef27be1ff09",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create output subdirectory",
   "id": "b9f4345adc617bce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:44:05.115635Z",
     "start_time": "2025-08-15T17:44:05.111916Z"
    }
   },
   "cell_type": "code",
   "source": "create_directory(path)",
   "id": "5c77a6049e029a22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph1_arianne/grp\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Capture a few document pages for inspection",
   "id": "12741feb2537795d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:44:07.825306Z",
     "start_time": "2025-08-15T17:44:07.819086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filename_sample_1 = 'greenpeace_stories_sample1.html'\n",
    "url_sample_1 = 'https://www.greenpeace.org/international/story/page/1/'\n",
    "filename_sample_11 = 'greenpeace_stories_sample11.html'\n",
    "url_sample_11 = 'https://www.greenpeace.org/international/story/77736/from-hiroshima-to-gaza-defending-peace/'\n",
    "filename_sample_2 = 'greenpeace_stories_sample2.html'\n",
    "url_sample_2 = 'https://www.greenpeace.org/international/story/page/2/'\n",
    "filename_sample_21 = 'greenpeace_stories_sample21.html'\n",
    "url_sample_21 = 'https://www.greenpeace.org/international/story/77406/boots-to-boost-justice-standing-in-solidarity-with-indonesian-migrant-fishers/'\n",
    "filename_sample_3 = 'greenpeace_stories_sample3.html'\n",
    "url_sample_3 = 'https://www.greenpeace.org/international/story/page/3/'\n",
    "filename_sample_31 = 'greenpeace_stories_sample31.html'\n",
    "url_sample_31 = 'https://www.greenpeace.org/international/story/76810/vanishing-millet-fields-endangered-sparrows-the-climate-crisis-and-taiwans-forgotten-guardians/'"
   ],
   "id": "b22eb22a0eed40cf",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:44:17.546455Z",
     "start_time": "2025-08-15T17:44:11.010029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "document_page_sample_1 = scrape_html(url_sample_1)\n",
    "\n",
    "with open(f'{path}/{filename_sample_1}', 'w', encoding='utf8', newline='\\n') as file:\n",
    "    file.write(document_page_sample_1)"
   ],
   "id": "5aa51a45833b3f03",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:44:25.659755Z",
     "start_time": "2025-08-15T17:44:19.320281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "document_page_sample_11 = scrape_html(url_sample_11)\n",
    "\n",
    "with open(f'{path}/{filename_sample_11}', 'w', encoding='utf8', newline='\\n') as file:\n",
    "    file.write(document_page_sample_11)"
   ],
   "id": "7882b7a20395a30a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:44:30.725088Z",
     "start_time": "2025-08-15T17:44:27.500888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "document_page_sample_2 = scrape_html(url_sample_2)\n",
    "\n",
    "with open(f'{path}/{filename_sample_2}', 'w', encoding='utf8', newline='\\n') as file:\n",
    "    file.write(document_page_sample_2)"
   ],
   "id": "31febd2f94481e3b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:44:40.095120Z",
     "start_time": "2025-08-15T17:44:32.539118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "document_page_sample_21 = scrape_html(url_sample_21)\n",
    "\n",
    "with open(f'{path}/{filename_sample_21}', 'w', encoding='utf8', newline='\\n') as file:\n",
    "    file.write(document_page_sample_21)"
   ],
   "id": "1d56dd409f7a82de",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:44:48.167837Z",
     "start_time": "2025-08-15T17:44:42.112427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "document_page_sample_3 = scrape_html(url_sample_3)\n",
    "\n",
    "with open(f'{path}/{filename_sample_3}', 'w', encoding='utf8', newline='\\n') as file:\n",
    "    file.write(document_page_sample_3)"
   ],
   "id": "ffb47da31973ab27",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T17:44:59.200627Z",
     "start_time": "2025-08-15T17:44:50.897899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "document_page_sample_31 = scrape_html(url_sample_31)\n",
    "\n",
    "with open(f'{path}/{filename_sample_31}', 'w', encoding='utf8', newline='\\n') as file:\n",
    "    file.write(document_page_sample_31)"
   ],
   "id": "e4b384b766018ee9",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Scraping the articles",
   "id": "9cd43188047cde5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T21:14:50.708523Z",
     "start_time": "2025-08-13T21:14:50.690034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "source = 'Greenpeace'\n",
    "index_page_url_1 = 'https://www.greenpeace.org/international/story/page/'\n",
    "index_page_url_2 = '/'\n",
    "start_page = 1\n",
    "end_page = 136"
   ],
   "id": "101525d95ff0cd1",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_grp = scrape_articles(source, index_page_url_1, index_page_url_2, start_page, end_page)",
   "id": "f97f09f562880a15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 6,
   "source": [
    "def scrape_articles(source, index_page_url_1, index_page_url_2, start_page, end_page):\n",
    "    data = []\n",
    "\n",
    "    for i in tqdm(range(start_page, end_page + 1)):\n",
    "        url = f\"{index_page_url_1}{i}{index_page_url_2}\"\n",
    "\n",
    "        index_page = scrape_html(url)\n",
    "\n",
    "        # Parse page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(index_page, 'lxml')\n",
    "\n",
    "        # Capture the listing page content\n",
    "        listing_page_content = soup.find('div', id='listing-page-content')\n",
    "\n",
    "        # Extract the items\n",
    "        if listing_page_content:\n",
    "            list = listing_page_content.find('ul', class_='wp-block-post-template')\n",
    "            if list:\n",
    "                items = list.find_all('li')\n",
    "\n",
    "        for item in items:\n",
    "            # Extract the item body\n",
    "            body = item.find('div', class_='query-list-item-body')\n",
    "\n",
    "            # Extract the post term\n",
    "            if body:\n",
    "                post_term = body.find('div', class_='wp-block-post-terms')\n",
    "                if post_term:\n",
    "                    post_term_text = ' '.join(post_term.get_text(' ', strip=True).split()) if post_term else ''\n",
    "\n",
    "            # Extract the post tags\n",
    "            if body:\n",
    "                post_tags = body.find('div', class_='taxonomy-post_tag wp-block-post-terms')\n",
    "                if post_tags:\n",
    "                    post_tags_list = [a.get_text(strip=True) for a in post_tags.select('a[rel=\"tag\"]')]\n",
    "                    post_tags_text = \", \".join(post_tags_list) if post_tags_list else ''\n",
    "\n",
    "            # Extract the title\n",
    "            if body:\n",
    "                headline = body.find('h4', class_='query-list-item-headline wp-block-post-title')\n",
    "                title_text = ' '.join(headline.get_text(' ', strip=True).split()) if headline else ''\n",
    "\n",
    "            # Extract the URL\n",
    "            if headline:\n",
    "                anchor = headline.find('a')\n",
    "                url = anchor['href'] if anchor else ''\n",
    "\n",
    "            # Extract the authors\n",
    "            authors_tag = article.find('input', class_='inputAuthor')\n",
    "            authors = authors_tag['value'] if authors_tag else ''\n",
    "\n",
    "            # Extract published date\n",
    "            published_tag = article.find('input', class_='inputEPubDate')\n",
    "            published = published_tag['value'] if published_tag else ''\n",
    "\n",
    "            # Extract DOI\n",
    "            doi = f\"{doi_root_url}{doi_tag['value']}\" if doi_tag else ''\n",
    "\n",
    "            # Extract Free Access status\n",
    "            free_access_tag = article.find('span', class_='issue-item_free')\n",
    "            free_access = free_access_tag.get_text(strip=True) if free_access_tag else ''\n",
    "\n",
    "            # Extract PDF URL\n",
    "            pdf_url = f\"{root_url}/doi/pdf/{doi_tag['value']}\" if doi_tag else ''\n",
    "\n",
    "            # Append extracted data\n",
    "            data.append({\n",
    "                'Article Type': article_type,\n",
    "                'Title': title,\n",
    "                'URL': article_url,\n",
    "                'Authors': authors,\n",
    "                'Vol/Issue': volume_issue,\n",
    "                'Published': published,\n",
    "                'DOI': doi,\n",
    "                'Free Access': free_access,\n",
    "                'PDF URL': pdf_url,\n",
    "                'Area of Knowledge': area_of_knowledge\n",
    "            })\n",
    "\n",
    "    # Close WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    return pd.DataFrame(data)"
   ],
   "id": "7e4ed3a6e89f7821"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:18<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "execution_count": 20,
   "source": [
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate through the URLs and using 'tqdm' for progress tracking in the range loop\n",
    "for i in tqdm(range(start_page, end_page + 1)):\n",
    "    url = f\"{article_list_url}{i}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "    # Find all <article> elements with the class 'u-full-height c-card c-card--flush'\n",
    "    articles = soup.find_all('article', class_='u-full-height c-card c-card--flush')\n",
    "\n",
    "    for article in articles:\n",
    "        # Extracting the title\n",
    "        title_tag = article.find('h3', class_='c-card__title').find('a')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else ''\n",
    "        title_url = f\"{root_url}{title_tag.get('href')}\" if title_tag else ''\n",
    "\n",
    "        # Extracting the authors\n",
    "        author_tags = article.find_all('li', itemprop='creator')\n",
    "        authors = ', '.join(author.get_text(strip=True) for author in author_tags)\n",
    "\n",
    "        # Extracting the published date\n",
    "        date_tag = article.find('time')\n",
    "        date_published = date_tag['datetime'] if date_tag else ''\n",
    "\n",
    "        # Extracting the PDF URL\n",
    "        pdf_url = f\"{title_url}.pdf\" if title_url else ''\n",
    "\n",
    "        # Extracting the 'Open Access' label\n",
    "        open_access_tag = article.find('span', class_='u-color-open-access')\n",
    "        open_access = open_access_tag.get_text(strip=True) if open_access_tag else ''\n",
    "\n",
    "        # Appending the data to the list\n",
    "        data.append({\n",
    "            'Title': title,\n",
    "            'URL': title_url,\n",
    "            'Authors (compact list)': authors,\n",
    "            'Published': date_published,\n",
    "            'PDF URL': pdf_url,\n",
    "            'Open Access': open_access,\n",
    "            'Area of Knowledge': area_of_knowledge\n",
    "        })\n",
    "\n",
    "# Creating a DataFrame from the data\n",
    "df_nature_food = pd.DataFrame(data)"
   ],
   "id": "f7326dfa4fd759bd"
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
