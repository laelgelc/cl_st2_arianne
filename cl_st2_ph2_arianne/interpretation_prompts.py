#!/usr/bin/env python3
"""
Generate interpretation prompt files for factor poles (f1_pos ... f7_neg).

For each pole, this script assembles a complete prompt containing:
1. System prompt
2. User prompt
3. Mean source scores
4. Factor loadings
5. Example excerpts (10 files × first 15 lines), with their loading words appended

The output is written to:
    interpretation/input/f<n>_<pole>.txt
"""

import re
from pathlib import Path

# ============================================================
# PATHS
# ============================================================
FACTORS_DIR = Path("factors")
EXAMPLES_DIR = Path("examples_txt")
DETAILS_FILE = Path("examples/score_details.txt")
MEANS_DIR = Path("sas/output_cl_st2_ph2_arianne")
OUTPUT_DIR = Path("interpretation/input")

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ============================================================
# PROMPTS
# ============================================================

SYSTEM_PROMPT = """You are a corpus linguist specializing in Lexical Multi-Dimensional Analysis (LMDA).
Your task is to interpret a single factor pole as a discourse dimension.

The corpus comes from an oral-history style interview project in which people living in the USA 
during the COVID-19 pandemic were interviewed. The corpus includes:
• Human interviewee responses
• Synthetic responses generated by GPT, Grok, and Gemini

The LLM-generated responses were produced under two prompting regimes:
1. A plain prompt, which instructed the model to speak as a person living in the USA 
   during the pandemic and respond to interview questions.
2. An enhanced prompt, which additionally supplied background demographic/biographical information 
   and a list of thematic points that the LLM should incorporate into its answers.

Your interpretation must identify the discourses encoded at this pole, taking into account:
• Linguistic loadings (which represent the entire corpus)
• The example excerpts (which illustrate high-scoring texts at this pole)
"""

USER_PROMPT = """Interpret Factor {factor} ({polarity}) as a discourse dimension. 
Propose possible labels for *this pole only* and justify them.

Base your interpretation on:
• Mean source scores (which sources drive this pole). For the positive poles, consider the highest scoring groups in the table. For the negative poles, consider the lowest scoring groups; these may be the lowest positive score or the highest negative scores if there are any.

• Factor loadings
• Example excerpts from high-scoring texts
• The loading words that appear in these examples
• Comment on which group (human or LLM and prompt type) is driving the pole.
• Don't offer a 'versus' dimension pole. Focus on this single pole only.

Give equal weight to the loadings and the examples. Remember that loadings represent the full corpus,
whereas the excerpts are only 10 high-scoring samples.
"""

# ============================================================
# LOAD SCORE DETAILS
# ============================================================

def load_score_details(details_path):
    """
    Parse score_details.txt into a structure:
        score_details[text_id][factor]["pos" or "neg"] = list_of_words
    """
    score_details = {}
    current_id = None
    current_factor = None

    with open(details_path, "r", encoding="utf8") as f:
        for line in f:
            line = line.strip()

            # "text ID: t000001"
            m = re.match(r"text ID:\s*(t\d+)", line)
            if m:
                current_id = m.group(1)
                score_details[current_id] = {}
                continue

            # "f2 score:"
            m = re.match(r"(f\d+)\s+score:", line)
            if m:
                current_factor = m.group(1)
                score_details[current_id][current_factor] = {"pos": [], "neg": []}
                continue

            # "f2 pos words (N=...): word, word"
            m = re.match(r"f\d+\s+(pos|neg)\s+words.*:\s*(.*)", line)
            if m and current_id and current_factor:
                pole = m.group(1)
                words_str = m.group(2).strip()
                words = [w.strip() for w in words_str.split(",") if w.strip()]
                score_details[current_id][current_factor][pole] = words

    return score_details


# ============================================================
# EXCERPT EXTRACTOR
# ============================================================

def extract_excerpt(file_path, n_lines=30):
    lines = []
    with open(file_path, "r", encoding="utf8") as f:
        for _ in range(n_lines):
            line = f.readline()
            if not line:
                break
            lines.append(line.rstrip("\n"))
    return "\n".join(lines)


# ============================================================
# MAIN SCRIPT
# ============================================================

def main():
    score_details = load_score_details(DETAILS_FILE)

    # Iterate over all factor-pole files: f1_pos.txt, f1_neg.txt, ...
    for factor_file in sorted(FACTORS_DIR.glob("f*_*.txt")):
        factor_name = factor_file.stem          # e.g. "f1_pos"
        factor = factor_name.split("_")[0]      # "f1"
        polarity = factor_name.split("_")[1]    # "pos" or "neg"

        # Load factor loadings text
        loadings_text = factor_file.read_text(encoding="utf8").strip()

        # Load mean scores
        means_file = MEANS_DIR / f"means_group_{factor}.tsv"
        if not means_file.exists():
            print(f"Warning: missing means file {means_file}")
            means_text = "(No means file found)"
        else:
            means_text = means_file.read_text(encoding="utf8").strip()

        # Example files (first 10)
        example_folder = EXAMPLES_DIR / factor_name
        example_files = sorted(example_folder.glob("*.txt"))[:10]

        excerpts_block = []

        # Build excerpt blocks with appended loading words
        for ex_path in example_files:
            excerpt = extract_excerpt(ex_path)
            file_text = ex_path.read_text(encoding="utf8")

            # detect tID inside the file
            m = re.search(r"(t0\d{5})", file_text)
            if not m:
                print(f"Warning: no text ID found in {ex_path}")
                continue
            tid = m.group(1)

            # loading words for this example
            loading_words = score_details.get(tid, {}).get(factor, {}).get(polarity, [])
            lw_string = ", ".join(loading_words) if loading_words else "(none)"

            excerpt_block = (
                f"\n===== EXCERPT: {ex_path.name} (text ID {tid}) =====\n"
                f"{excerpt}\n"
                f"\n--- Loading words for this example ({polarity}): {lw_string}\n"
            )
            excerpts_block.append(excerpt_block)

        # Assemble final prompt
        system_final = SYSTEM_PROMPT
        user_final = USER_PROMPT.format(factor=factor, polarity=polarity)
        mean_final = f"\n=== MEAN SOURCE SCORES ===\n{means_text}\n"
        loadings_final = f"\n=== FACTOR LOADINGS ({factor_name}) ===\n{loadings_text}\n"

        final_prompt = (
            system_final
            + "\n\n"
            + user_final
            + "\n"
            + mean_final
            + loadings_final
            + "\n=== EXAMPLE EXCERPTS ===\n"
            + "\n".join(excerpts_block)
        )

        # Save output file
        out_path = OUTPUT_DIR / f"{factor_name}.txt"
        out_path.write_text(final_prompt, encoding="utf8")

        print("Wrote:", out_path)


if __name__ == "__main__":
    main()
